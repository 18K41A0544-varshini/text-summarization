# -*- coding: utf-8 -*-
"""textsummarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TD4WnUWbsKtvD9T808rwzn-77ftR0-d1
"""

text1= """
Extractive summarization is a challenging task that has only recently become practical.
Like many things NLP, one reason for this progress is the superior embeddings offered by transformer models like BERT.
This project uses BERT sentence embeddings to build an extractive summarizer taking two supervised approaches.
The first considers only embeddings and their derivatives.
This corresponds to our intuition that a good summarizer can parse meaning and should select sentences based purely on the internal structure of the article. The baseline for this approach is the unsupervised TextRank model. The other approach incorporates sequential information and takes advantage of the well known Lead3 phenomena particular to news corpuses. This is the observation that the first three sentences typically do a good job in summarizing the article. In fact, this strategy is explicitly deployed by many publishers. Lead3 is used as the baseline for this second approach.
In both cases, the supervised models outperform the baselines on the Rouge-1 and Rouge-L F1 metric."""

"""test example

"""

import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation

"""Importing libraries"""

stopwords = list(STOP_WORDS)

nlp = spacy.load('en_core_web_sm')

document = nlp(text1)

tokens = [token.text for token in document]
print(tokens)

"""converting each word of sentence into tokens"""

punctuation = punctuation + '\n'
punctuation

"""Finding punctuations in the paragraph"""

word_freq = {}
for word in document:
    if word.text.lower() not in stopwords:
        if word.text.lower() not in punctuation:
            if word.text not in word_freq.keys():
                word_freq[word.text] = 1
            else:
                word_freq[word.text] += 1
                
print(word_freq)

"""Frequency of each word in the text"""

max_freq = max(word_freq.values())
max_freq

"""Finding maximum frequency"""

for word in word_freq.keys():
    word_freq[word] = word_freq[word]/max_freq

print(word_freq)

"""Percentage of each word in the text"""

sen_tokens = [sent for sent in document.sents]
print(sen_tokens)

"""Dividing sentences into tokens with fullstops"""

sen_scores = {}
for sent in sen_tokens:
    for word in sent:
        if word.text.lower() in word_freq.keys():
            if sent not in sen_scores.keys():
                sen_scores[sent] = word_freq[word.text.lower()]
            else:
                sen_scores[sent] += word_freq[word.text.lower()]
                
sen_scores

"""Alloting score to each sentence"""

from heapq import nlargest

length = int(len(sen_tokens)*0.3)
length

summary = nlargest(length, sen_scores, key = sen_scores.get)
summary

"""Finding the sentences with highest scores"""

final_summary = [word.text for word in summary]
summary = ' '.join(final_summary)

"""Joining sentences with highest score"""

print(text1)

print(summary)

len(summary)

len(text1)

